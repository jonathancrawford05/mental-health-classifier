{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mental Health Classifier - Exploration Notebook\n",
    "\n",
    "This notebook demonstrates the mental health classifier for Depression, Anxiety, and Suicide risk detection.\n",
    "\n",
    "## Features\n",
    "- Multi-headed attention transformer architecture\n",
    "- Clinical text preprocessing\n",
    "- Attention visualization\n",
    "- Performance analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path('..').resolve()\n",
    "if str(project_root / 'src') not in sys.path:\n",
    "    sys.path.append(str(project_root / 'src'))\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set style\n",
    "plt.style.use('default')\n",
    "sns.set_palette('husl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import project modules\n",
    "from src.models import MentalHealthClassifier, create_model\n",
    "from src.data import DataProcessor, ClinicalTextPreprocessor, create_sample_data\n",
    "from src.training import create_trainer\n",
    "from src.utils import (\n",
    "    load_config, setup_logging, set_random_seeds, get_device,\n",
    "    print_model_summary, print_data_summary\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load configuration\n",
    "config = load_config('../config/config.yaml')\n",
    "\n",
    "# Setup\n",
    "set_random_seeds(42)\n",
    "device = get_device()\n",
    "\n",
    "print(f'Using device: {device}')\n",
    "print(f'Configuration loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Sample Data\n",
    "\n",
    "Let's create some sample data to experiment with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create sample dataset\n",
    "os.makedirs('../data', exist_ok=True)\n",
    "create_sample_data('../data/sample_data.csv', num_samples=300)\n",
    "\n",
    "# Load and examine the data\n",
    "df = pd.read_csv('../data/sample_data.csv')\n",
    "print(f'Dataset shape: {df.shape}')\n",
    "print('\\nLabel distribution:')\n",
    "print(df['label'].value_counts())\n",
    "\n",
    "# Show some examples\n",
    "print('\\nSample texts:')\n",
    "for label in df['label'].unique():\n",
    "    print(f'\\n{label.upper()}:')\n",
    "    examples = df[df['label'] == label]['text'].head(2)\n",
    "    for i, text in enumerate(examples, 1):\n",
    "        print(f'{i}. {text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing\n",
    "\n",
    "Let's explore the clinical text preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Initialize preprocessor\n",
    "preprocessor = ClinicalTextPreprocessor(\n",
    "    expand_contractions=True,\n",
    "    normalize_clinical_terms=True\n",
    ")\n",
    "\n",
    "# Test preprocessing\n",
    "sample_text = \"Pt c/o severe depression w/ SI and h/o MDD. R/O GAD.\"\n",
    "\n",
    "print('Original text:')\n",
    "print(sample_text)\n",
    "\n",
    "print('\\nPreprocessed text:')\n",
    "print(preprocessor.preprocess(sample_text))\n",
    "\n",
    "print('\\nTokenized:')\n",
    "print(preprocessor.tokenize_text(sample_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture\n",
    "\n",
    "Let's create and examine the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Update config for small example\n",
    "config['model'].update({\n",
    "    'vocab_size': 1000,  # Will be updated after building vocab\n",
    "    'n_embd': 128,\n",
    "    'num_heads': 4,\n",
    "    'n_layer': 3,\n",
    "    'max_seq_length': 256\n",
    "})\n",
    "\n",
    "# Create model\n",
    "model = create_model(config['model'])\n",
    "\n",
    "# Print model summary\n",
    "print_model_summary(model, config['model'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Split data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['label'])\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42, stratify=train_df['label'])\n",
    "\n",
    "print(f'Train: {len(train_df)}, Val: {len(val_df)}, Test: {len(test_df)}')\n",
    "\n",
    "# Initialize data processor\n",
    "data_processor = DataProcessor(config['data'])\n",
    "\n",
    "# Prepare data\n",
    "train_texts = train_df['text'].tolist()\n",
    "train_labels = data_processor.encode_labels(train_df['label'].tolist())\n",
    "\n",
    "val_texts = val_df['text'].tolist()\n",
    "val_labels = data_processor.encode_labels(val_df['label'].tolist())\n",
    "\n",
    "test_texts = test_df['text'].tolist()\n",
    "test_labels = data_processor.encode_labels(test_df['label'].tolist())\n",
    "\n",
    "# Build vocabulary\n",
    "data_processor.build_vocabulary(train_texts)\n",
    "\n",
    "# Update model vocab size\n",
    "config['model']['vocab_size'] = len(data_processor.vocab)\n",
    "model = create_model(config['model'])\n",
    "\n",
    "print(f'Vocabulary size: {len(data_processor.vocab)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Let's train a small model for demonstration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create data loaders\n",
    "dataloaders = data_processor.create_dataloaders(\n",
    "    train_texts, train_labels,\n",
    "    val_texts, val_labels,\n",
    "    test_texts, test_labels\n",
    ")\n",
    "\n",
    "# Update training config for quick demo\n",
    "config['training'].update({\n",
    "    'num_epochs': 3,\n",
    "    'batch_size': 16,\n",
    "    'learning_rate': 1e-3\n",
    "})\n",
    "\n",
    "# Calculate class weights\n",
    "class_weights = data_processor.get_class_weights(train_labels)\n",
    "\n",
    "# Create trainer\n",
    "trainer = create_trainer(\n",
    "    model=model,\n",
    "    config=config['training'],\n",
    "    device=device,\n",
    "    class_weights=class_weights\n",
    ")\n",
    "\n",
    "print('Starting training...')\n",
    "trainer.train(dataloaders['train'], dataloaders['val'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Evaluate on test set\n",
    "test_metrics = trainer.evaluate(dataloaders['test'], save_plots=False)\n",
    "\n",
    "print('\\nTest Results:')\n",
    "for metric, value in test_metrics.items():\n",
    "    if isinstance(value, (int, float)):\n",
    "        print(f'{metric}: {value:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot training history\n",
    "trainer.plot_training_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Test predictions\n",
    "test_examples = [\n",
    "    \"I feel completely hopeless and nothing seems to matter anymore\",\n",
    "    \"I'm constantly worried about everything and can't stop the racing thoughts\",\n",
    "    \"I've been having thoughts about ending my life\",\n",
    "    \"Patient reports feeling great and enjoying activities\"\n",
    "]\n",
    "\n",
    "print('PREDICTION EXAMPLES')\n",
    "print('=' * 50)\n",
    "\n",
    "for text in test_examples:\n",
    "    prediction, probabilities = trainer.predict_text(\n",
    "        text, data_processor, return_probabilities=True\n",
    "    )\n",
    "    \n",
    "    print(f'Text: {text}')\n",
    "    print(f'Predicted: {prediction}')\n",
    "    print('Probabilities:')\n",
    "    for label, prob in probabilities.items():\n",
    "        print(f'  {label}: {prob:.3f}')\n",
    "    print('-' * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Visualization\n",
    "\n",
    "Let's visualize what the model is paying attention to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Get attention weights for a sample\n",
    "sample_text = \"I feel hopeless and have thoughts of ending my life\"\n",
    "\n",
    "# Preprocess and tokenize\n",
    "preprocessed = data_processor.preprocessor.preprocess(sample_text)\n",
    "tokens = data_processor.tokenizer(preprocessed)\n",
    "token_ids = [data_processor.vocab[token] for token in tokens]\n",
    "\n",
    "# Pad if necessary\n",
    "max_length = config['model']['max_seq_length']\n",
    "if len(token_ids) > max_length:\n",
    "    token_ids = token_ids[:max_length]\n",
    "    tokens = tokens[:max_length]\n",
    "\n",
    "# Create attention mask\n",
    "attention_mask = [1] * len(token_ids)\n",
    "pad_token_id = data_processor.vocab['<pad>']\n",
    "padding_length = max_length - len(token_ids)\n",
    "token_ids.extend([pad_token_id] * padding_length)\n",
    "attention_mask.extend([0] * padding_length)\n",
    "\n",
    "# Convert to tensors\n",
    "input_ids = torch.tensor([token_ids], dtype=torch.long).to(device)\n",
    "attention_mask_tensor = torch.tensor([attention_mask], dtype=torch.long).to(device)\n",
    "\n",
    "# Get attention weights\n",
    "attention_weights = model.get_attention_weights(input_ids, attention_mask_tensor)\n",
    "\n",
    "# Visualize attention for first layer, first head\n",
    "if attention_weights:\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Get attention for actual tokens (not padding)\n",
    "    seq_len = len(tokens)\n",
    "    attn = attention_weights[0][0, 0, :seq_len, :seq_len].cpu().detach().numpy()\n",
    "    \n",
    "    plt.imshow(attn, cmap='Blues', aspect='auto')\n",
    "    plt.colorbar()\n",
    "    plt.title('Attention Weights - Layer 1, Head 1')\n",
    "    plt.xlabel('Key Position')\n",
    "    plt.ylabel('Query Position')\n",
    "    \n",
    "    # Add token labels\n",
    "    if len(tokens) <= 15:  # Only for short sequences\n",
    "        plt.xticks(range(len(tokens)), tokens, rotation=45)\n",
    "        plt.yticks(range(len(tokens)), tokens)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f'Sample text: {sample_text}')\n",
    "    print(f'Tokens: {tokens}')\n",
    "    print(f'Attention shape: {attention_weights[0].shape}')\n",
    "else:\n",
    "    print('No attention weights available')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Analysis\n",
    "\n",
    "Let's analyze model performance in more detail:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Analyze predictions by class\n",
    "from collections import defaultdict\n",
    "\n",
    "class_predictions = defaultdict(list)\n",
    "class_correct = defaultdict(int)\n",
    "class_total = defaultdict(int)\n",
    "\n",
    "# Get predictions for test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in dataloaders['test']:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        predictions = torch.argmax(outputs['logits'], dim=-1)\n",
    "        \n",
    "        for pred, label in zip(predictions.cpu().numpy(), labels.cpu().numpy()):\n",
    "            class_predictions[label].append(pred)\n",
    "            class_total[label] += 1\n",
    "            if pred == label:\n",
    "                class_correct[label] += 1\n",
    "\n",
    "# Print per-class accuracy\n",
    "print('Per-class Performance:')\n",
    "print('-' * 30)\n",
    "for class_id in sorted(class_total.keys()):\n",
    "    class_name = data_processor.label_names[class_id]\n",
    "    accuracy = class_correct[class_id] / class_total[class_id]\n",
    "    print(f'{class_name}: {accuracy:.3f} ({class_correct[class_id]}/{class_total[class_id]})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "This notebook demonstrates the basic functionality of the mental health classifier. For production use, consider:\n",
    "\n",
    "1. **Larger datasets**: Use real clinical datasets like MIMIC-III/IV\n",
    "2. **Model scaling**: Increase model size and training epochs\n",
    "3. **Clinical validation**: Validate with mental health professionals\n",
    "4. **Ethical considerations**: Implement safeguards and bias detection\n",
    "5. **Integration**: Connect with clinical decision support systems\n",
    "\n",
    "**Important**: This is a research tool and should not be used for actual clinical diagnosis without proper validation and oversight."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
